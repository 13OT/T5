{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8294, 0.4467, 0.6588],\n",
      "        [0.9598, 0.9173, 0.2108]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4102, 0.4272, 0.5989],\n",
      "        [0.0288, 0.1383, 0.2290]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mystopwords = ['ltc', 'product', 'ought', 'gorgeous', 'time', 'btc', 'btc ', 'bitcoin', 'bitcoin ', 'revoluciónde', 'free', 'coincheck', 'dash', 'usd', 'post', 'cryptoworld', 'ico', 'powell', 'eth', 'doge', 'xrp', 'xpc', 'price ', 'crypto', 'buy', 'market', 'amp', 'sell', 'bch', 'cryptocurrency', 'money', 'like', 'news', 'volume', 'new', 'earn', 'exchange', 'current', 'mine', 'cash', 'coin', 'blockchain', 'go', 'join', 'good', 'jpy', 'people', 'link', 'start', 'ethereum', 'world', 'change', 'think', 'xlm', 'twitter', 'look', 'value', 'project', 'know', 'binance', 'use', 'come', 'long', 'eos', 'short', 'pay', 'worth', 'need', 'want', 'block', 'day', 'today', 'invest', 'get', 'daily', 'best', 'say', 'profit', 'thank', 'wallet', 'gold', 'neo', 'great', 'info', 'que', 'bank', 'trx', 'account', 'check', 'read', 'bnb', 'noticias', 'update', 'xmr', 'follow', 'usdt', 'forex', 'ada', 'days', 'support', 'currency', 'fee', 'work', 'hour', 'right', 'signal', 'utc', 'average', 'investment', 'way', 'analysis', 'sign', 'help', 'hold', 'high', 'tweet', 'break', 'com', 'eur', 'future', 'cann', 'emc', 'btg', 'extremely', 'hard', 'truth', 'rule', 'busy', 'jerome', 'hace', 'caer', 'ask', 'bid', 'alert', 'currently', 'round', 'participants', 'hereistitle', 'setup', 'automatic', 'bitcome', 'automatically', 'businesses', 'minutes', 'sleep', 'idol', 'sit', 'generate', 'bitco', 'aud', 'trxprice', 'discount', 'feereceive', 'coinbasewaves', 'grs', 'xvg', 'mco', 'omg', 'btcd', 'devastadoras', 'para', 'planeta', 'menos', 'décadas', 'xem', 'lsk', 'mona', 'number', 'miner', 'unknown', 'transactions', 'size', 'bytes', 'total', 'supply', 'issue', '草コイン', '仮想通貨', 'cap', 'billion', 'base', 'bpi', 'cloud', 'gominer', 'instant', 'witdraw', 'litecoin', 'iota', 'channel', 'ocn', 'iost', 'ncash', 'storm', 'cdt', 'ven', 'gvt', 'lisk', 'appc', 'mod', 'nebl', 'enj', 'precio', 'fuente', 'hora', 'gmt', 'satoshi', 'waste', 'register', 'payouts', 'verify', 'valor', 'aumentou', 'caiu', 'actual', 'del', 'mxn', 'compra', 'vende', 'ripple', 'desde', 'aqu', 'unique', 'modern', 'win', 'strategies', 'develop', 'directly', 'traders', 'company', 'low', 'ビットコイン', 'bitflyer', 'spot', 'bitmex', 'finex', 'frr', 'smart', 'trader', 'bot', 'andtrailing', 'stop', 'loss', 'pairsauto', 'renew', 'ordersprofit', 'notifications', 'order', 'liquidation', 'perpetualsold', 'jul', 'btcこれによって現在が', '波にしてもc', 'app', 'store', 'catch', 'chance', 'dappt', 'listingvia', 'mercadobitcoin', 'のbtc', 'note第', 'fibonacci', 'fib', 'btcac', 'btcというのもここは前回高値', 'btc逆三尊から開始した上昇', '付近で', 'finex過去の高値は', 'これは日足の雲ですが', 'btcにおいて', '最初にタッチした', '月から', 'さらに', '課題は', 'bybit', 'jun', 'clear', 'task', 'bonus', 'pool', 'participant', 'referal', '週足ベースでの大きな下落後のw', 'report', 'miss', 'close', 'registrations', 'zcl', 'zec', 'snt', 'nxt', 'powr', 'vtc', 'vox', 'dgb', 'coval', 'strat', 'xrb', 'nav', 'weakly', 'trend', 'momentum', 'suggest', 'neutral', 'visit', 'spd', 'yobit', 'hitbtc', 'livecoin', 'dominance', 'max', 'mini', 'brl', 'perpetualbought', 'site', 'terminal', 'large', 'transaction', 'url', 'return', 'asia', 'texas', 'poker', 'sss', 'neteller', 'special', 'rake', 'date', 'power', 'dolar', 'bss', 'nuevos', 'indicadores', 'euro', 'peso', 'col', 'sole', 'chile', 'ここから', '年のように', 'serbest', 'tarih', 'ayar', 'gram', 'çeyrek', 'yarım', 'tam', 'プレゼント', '売買サイン通知', 'bitstamp', 'bitfinex', 'coinbase', 'chxp', 'marvellous', 'maintenance', 'child', 'trade', 'week', 'real', 'network', 'big', 'chart', 'digital', 'run', 'try', 'hit', 'let', 'platform', 'bull', 'make', 'years', 'send', 'leverage', 'year', 'launch', 'bsv', 'address', 'bullish', 'better', 'move', 'drop', 'token', 'million', 'pump', 'latest', 'alts', 'gain', 'interest', 'live', 'receive', 'take', 'watch', 'happen', 'learn', 'guy', 'libra', 'fiat', 'lose', 'give', 'open', 'facebook', 'share', 'offer', 'level', 'wait', 'create', 'mean', 'end', 'btcusd', 'accept', 'dollar', 'tell', 'love', 'list', 'play', 'global', 'game', 'soon', 'increase', 'rise', 'term', 'point', 'talk', 'believe', 'team', 'stock', 'fund', 'hours', 'lot', 'china', 'target', 'futures', 'build', 'telegram', 'add', 'click', 'tokens', 'price', 'source', 'amaze', 'percent', 'lottery', 'jackpot', 'ready', 'zero', 'tradingwin', 'suffer', 'remotely', 'toggle', 'home', 'revolution', 'huge', 'intial', 'model', 'ñol', 'invitation', 'extra', 'instantly', 'recover', 'airdrop', 'act', 'vtho', 'mention', 'website', 'whale', 'alertsomeone', 'marketcap', 'fix', 'summary', 'volatility', 'rank', 'avg', 'pacific', 'height', 'nice', 'you']\n",
    "\n",
    "punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'         # define a string of punctuation symbols\n",
    "\n",
    "# Functions to clean tweets\n",
    "def remove_links(tweet):\n",
    "    \"\"\"Takes a string and removes web links from it\"\"\"\n",
    "    tweet = re.sub(r'http\\S+', 'http', tweet)   # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', 'http', tweet)  # remove bitly links\n",
    "    tweet = tweet.strip('[link]')   # remove [links]\n",
    "    tweet = re.sub(r'pic.twitter\\S+','http', tweet)\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '@', tweet)  # remove re-tweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '@', tweet)  # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    \"\"\"Takes a string and removes any hash tags\"\"\"\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hash tags\n",
    "    return tweet\n",
    "\n",
    "def remove_av(tweet):\n",
    "    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n",
    "    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n",
    "    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n",
    "    return tweet\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Returns tokenized representation of words in lemma form excluding stopwords\"\"\"\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(tweet):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in mystopwords \\\n",
    "                and len(token) > 2:  # drops words with less than 3 characters\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n",
    "\n",
    "def lemmatize(token):\n",
    "    \"\"\"Returns lemmatization of a token\"\"\"\n",
    "    return WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    \"\"\"Main master function to clean tweets, stripping noisy characters, and tokenizing use lemmatization\"\"\"\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = remove_hashtags(tweet)\n",
    "    tweet = remove_av(tweet)\n",
    "    tweet = re.sub('[' + punctuation + ']+', ' ', tweet)  # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  # remove double spacing\n",
    "    tweet = re.sub(\"[^a-zb(A-Z0-9]+ \", \"\", tweet)\n",
    "    return tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 12350,   363, 17841, 27969,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "[0.00760985 0.14581175 0.84657836]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import torch\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "device ='cuda:0'\n",
    "task = 'sentiment'\n",
    "MODEL = f\"./twitter-roberta-base-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels = []\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n",
    "model = torch.nn.DataParallel(model, device_ids=[0])\n",
    "\n",
    "\n",
    "text = \"Good night 😊\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "print(encoded_input)\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].cpu().detach().numpy()\n",
    "scores = softmax(scores)\n",
    "print(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night 😊\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].cpu().detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model('./lid.176.bin')\n",
    "def check_lang(x):\n",
    "    if model.predict(x)[0][0].split('_')[-1] == 'en':\n",
    "        return x\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv('tweets_cleaned.csv',index_col=0)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018= pd.DataFrame(df[(df.timestamp > \"2017-12-31 23:59\") &\n",
    "                   (df.timestamp < \"2019-01-01 00:00\")].sort_values('timestamp'))\n",
    "df2018.user.value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = pd.DataFrame(df2018.user.value_counts())\n",
    "vals.reset_index(inplace=True)\n",
    "vals.rename({'index': 'user', 'user': 'count'}, inplace=True, axis=1)\n",
    "repeated_users = vals.head(25).user.to_list()\n",
    "def check(x):\n",
    "    if x in repeated_users:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.user=df2018.user.apply(lambda x: check(x))\n",
    "df2018.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018['pp_text'] = df2018.text.apply(lambda x: preprocess_tweet(x))\n",
    "df2018.head(20)\n",
    "df2018.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.pp_text.value_counts().head(50)\n",
    "df2018.text.value_counts().head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_text = [x for x in df2018.pp_text.value_counts().head(50).index]\n",
    "df2018.pp_text.replace(rep_text,np.nan,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.isna().sum()\n",
    "df2018.dropna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.pp_text = df2018.pp_text.apply(lambda x: check_lang(x))\n",
    "df2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.dropna(subset=['user','pp_text'],axis=0,inplace=True)\n",
    "df2018.drop('text',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.reset_index(drop=True,inplace=True)\n",
    "rep_pptext = df2018.groupby(['user','pp_text']).count().sort_values('timestamp',ascending=False).head(100).reset_index().pp_text\n",
    "df2018.pp_text.replace(rep_pptext.to_list(),np.nan,inplace=True)\n",
    "df2018.dropna(subset=['user','pp_text'],axis=0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df2018.apply(lambda row: get_sentiment(row.pp_text), axis='columns', result_type='expand')\n",
    "df2018 = pd.concat([df2018, temp], axis='columns')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.rename({0: 'negative', 1: 'netural',2:'positive'}, inplace=True, axis=1)\n",
    "df2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bots =df2018[df2018.user.str.contains('bot')].user.value_counts().head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2018.user.replace(bots.index,np.nan,inplace=True)\n",
    "df2018.dropna(0,subset=['user'],inplace=True)\n",
    "df2018.drop('pp_text',inplace=True,axis=1)\n",
    "btc_df = pd.read_csv('./BTC-USD 2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailiy_df = df2018.groupby(df.timestamp.dt.date).agg(\n",
    "    Replies=('replies', 'sum'),Likes=('likes', 'sum'),Retweets=('retweets', 'sum'),Negative=('negative', 'mean'),Netural=('netural', 'mean'), Positive=('positive', 'mean'),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailiy_df.reset_index(inplace=True)\n",
    "dailiy_df.rename({'timestamp':'Date'},inplace=True,axis=1)\n",
    "dailiy_df.columns\n",
    "dailiy_df.Date = pd.to_datetime(dailiy_df.Date)\n",
    "btc_df.Date = pd.to_datetime(btc_df.Date)\n",
    "dailiy_df = pd.merge(dailiy_df, btc_df, on='Date')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz\n",
    "d= sweetviz.analyze(dailiy_df)\n",
    "d.show_html('FinalReport.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "AV = AutoViz_Class()\n",
    "av_d = AV.AutoViz('dailiy_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.DataFrame(dailiy_df)\n",
    "testdf.drop(['Open','High','Low','Volume','Netural','Replies','Likes','Adj Close'],axis=1,inplace=True)\n",
    "train = testdf[testdf.Date <'2018-11-01']\n",
    "test = testdf[testdf.Date >='2018-11-01']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret import *\n",
    "s = setup(data = train, test_data = test, target = 'Close', fold_strategy = 'timeseries', numeric_features = ['Negative','Retweets','Positive'], fold = 3, transform_target = True, session_id = 123)\n",
    "best = compare_models()\n",
    "prediction_holdout = predict_model(best)\n",
    "predictions = predict_model(best, data=testdf)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "predictions['Date'] = pd.date_range(start='2018-01-01', end = '2018-12-31')\n",
    "# line plot\n",
    "fig = px.line(predictions, x='Date', y=[\"Close\", \"Label\"], template = 'plotly_dark')\n",
    "# add a vertical rectange for test-set separation\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b69a98d3df882577ba469635c4ab08c5ae67eaedfd3a57f311f98966a6edb2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
